{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from cvae import CVAE, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y_cond, y_prop):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cond = torch.tensor(y_cond, dtype=torch.float32)\n",
    "        self.y_prop = torch.tensor(y_prop, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y_cond[i], self.y_prop[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54415a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_epoch(model, loader, opt, device,\n",
    "                epoch, warmup_epochs=120,\n",
    "                beta_max=1.0, lambda_prop=1.0):\n",
    "    model.train()\n",
    "\n",
    "    # Î² warm-up schedule\n",
    "    beta = beta_max * min(1.0, (epoch + 1) / warmup_epochs)\n",
    "\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "\n",
    "    for Xb, ycb, ypb in loader:\n",
    "        Xb, ycb, ypb = Xb.to(device), ycb.to(device), ypb.to(device)\n",
    "\n",
    "        X_hat, y_prop_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, ycb)\n",
    "\n",
    "        # Loss terms\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop = nn.functional.mse_loss(y_prop_pred, ypb, reduction=\"mean\")\n",
    "\n",
    "        loss = recon + beta * kl + lambda_prop * prop\n",
    "\n",
    "        # Backprop\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        # Logging\n",
    "        recon_losses.append(recon.item())\n",
    "        kl_losses.append(kl.item())\n",
    "        prop_losses.append(prop.item())\n",
    "\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses), beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, lambda_prop=1.0):\n",
    "    model.eval()\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "\n",
    "    for Xb, ycb, ypb in loader:\n",
    "        Xb, ycb, ypb = Xb.to(device), ycb.to(device), ypb.to(device)\n",
    "\n",
    "        X_hat, y_prop_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, ycb)\n",
    "\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop = nn.functional.mse_loss(y_prop_pred, ypb, reduction=\"mean\")\n",
    "\n",
    "        recon_losses.append(recon.item())\n",
    "        kl_losses.append(kl.item())\n",
    "        prop_losses.append(prop.item())\n",
    "\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c316473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Script\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load preprocessed data\n",
    "    data = np.load(\"../data/processed/data_splits.npz\")\n",
    "    X_train, X_val = data[\"X_train\"], data[\"X_val\"]\n",
    "    y_cond_train, y_cond_val = data[\"y_cond_train\"], data[\"y_cond_val\"]\n",
    "    y_prop_train, y_prop_val = data[\"y_prop_train\"], data[\"y_prop_val\"]\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(TabDataset(X_train, y_cond_train, y_prop_train),\n",
    "                              batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(TabDataset(X_val, y_cond_val, y_prop_val),\n",
    "                            batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    x_dim = X_train.shape[1]\n",
    "    y_cond_dim = y_cond_train.shape[1]\n",
    "    y_prop_dim = y_prop_train.shape[1]\n",
    "    model = CVAE(x_dim=x_dim, y_cond_dim=y_cond_dim, y_prop_dim=y_prop_dim,\n",
    "                 z_dim=4, hidden=128).to(device)\n",
    "\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    # Training loop\n",
    "    best_val = 1e9\n",
    "    patience, bad = 60, 0\n",
    "    EPOCHS = 200\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        tr_recon, tr_kl, tr_prop, beta = train_epoch(\n",
    "            model, train_loader, opt, device,\n",
    "            epoch, warmup_epochs=120, beta_max=1.0, lambda_prop=1.0\n",
    "        )\n",
    "        va_recon, va_kl, va_prop = eval_epoch(model, val_loader, device, lambda_prop=1.0)\n",
    "\n",
    "        print(f\"Ep {epoch:03d} | beta={beta:.3f} | \"\n",
    "              f\"train R {tr_recon:.4f} KL {tr_kl:.4f} P {tr_prop:.4f} | \"\n",
    "              f\"val R {va_recon:.4f} KL {va_kl:.4f} P {va_prop:.4f}\")\n",
    "\n",
    "        # Early stopping on (recon + prop)\n",
    "        val_score = va_recon + va_prop\n",
    "        if val_score < best_val - 1e-5:\n",
    "            best_val = val_score\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), \"../models/cvae_best.pt\")\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8842866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 000 | beta=0.008 | train R 0.9850 KL 0.2505 P 0.0955 | val R 1.0165 KL 0.4006 P 0.0657\n",
      "Ep 001 | beta=0.017 | train R 0.9196 KL 0.8059 P 0.0446 | val R 0.9299 KL 1.1986 P 0.0504\n",
      "Ep 002 | beta=0.025 | train R 0.7924 KL 1.9193 P 0.0371 | val R 0.8143 KL 2.1898 P 0.0396\n",
      "Ep 003 | beta=0.033 | train R 0.6818 KL 2.5110 P 0.0289 | val R 0.7096 KL 2.5562 P 0.0407\n",
      "Ep 004 | beta=0.042 | train R 0.5997 KL 2.8608 P 0.0287 | val R 0.6675 KL 2.6972 P 0.0328\n",
      "Ep 005 | beta=0.050 | train R 0.5638 KL 2.7443 P 0.0250 | val R 0.6357 KL 2.4682 P 0.0217\n",
      "Ep 006 | beta=0.058 | train R 0.5551 KL 2.5383 P 0.0222 | val R 0.6008 KL 2.5052 P 0.0281\n",
      "Ep 007 | beta=0.067 | train R 0.5365 KL 2.4971 P 0.0261 | val R 0.5976 KL 2.4773 P 0.0240\n",
      "Ep 008 | beta=0.075 | train R 0.5069 KL 2.4326 P 0.0208 | val R 0.5426 KL 2.3877 P 0.0224\n",
      "Ep 009 | beta=0.083 | train R 0.5078 KL 2.4176 P 0.0198 | val R 0.5385 KL 2.5060 P 0.0342\n",
      "Ep 010 | beta=0.092 | train R 0.5042 KL 2.3180 P 0.0225 | val R 0.5441 KL 2.2851 P 0.0213\n",
      "Ep 011 | beta=0.100 | train R 0.4790 KL 2.1906 P 0.0179 | val R 0.5287 KL 2.1519 P 0.0213\n",
      "Ep 012 | beta=0.108 | train R 0.4897 KL 2.0084 P 0.0188 | val R 0.5581 KL 1.9833 P 0.0285\n",
      "Ep 013 | beta=0.117 | train R 0.4979 KL 1.9101 P 0.0202 | val R 0.5178 KL 2.1280 P 0.0312\n",
      "Ep 014 | beta=0.125 | train R 0.4594 KL 1.9924 P 0.0202 | val R 0.5106 KL 1.9260 P 0.0288\n",
      "Ep 015 | beta=0.133 | train R 0.5080 KL 1.7252 P 0.0218 | val R 0.5259 KL 1.8734 P 0.0217\n",
      "Ep 016 | beta=0.142 | train R 0.5048 KL 1.8060 P 0.0229 | val R 0.5043 KL 1.8928 P 0.0238\n",
      "Ep 017 | beta=0.150 | train R 0.4726 KL 1.7313 P 0.0210 | val R 0.5325 KL 1.6615 P 0.0197\n",
      "Ep 018 | beta=0.158 | train R 0.5299 KL 1.6089 P 0.0186 | val R 0.5252 KL 1.8303 P 0.0258\n",
      "Ep 019 | beta=0.167 | train R 0.4922 KL 1.7966 P 0.0204 | val R 0.5067 KL 1.6926 P 0.0291\n",
      "Ep 020 | beta=0.175 | train R 0.5260 KL 1.5513 P 0.0200 | val R 0.5725 KL 1.5262 P 0.0236\n",
      "Ep 021 | beta=0.183 | train R 0.5217 KL 1.4788 P 0.0202 | val R 0.5394 KL 1.5426 P 0.0265\n",
      "Ep 022 | beta=0.192 | train R 0.5306 KL 1.5366 P 0.0198 | val R 0.5559 KL 1.5501 P 0.0193\n",
      "Ep 023 | beta=0.200 | train R 0.5369 KL 1.4174 P 0.0220 | val R 0.5657 KL 1.4132 P 0.0274\n",
      "Ep 024 | beta=0.208 | train R 0.5690 KL 1.2469 P 0.0207 | val R 0.6005 KL 1.3132 P 0.0281\n",
      "Ep 025 | beta=0.217 | train R 0.5425 KL 1.2890 P 0.0182 | val R 0.6066 KL 1.4267 P 0.0231\n",
      "Ep 026 | beta=0.225 | train R 0.5723 KL 1.2601 P 0.0197 | val R 0.4991 KL 1.2982 P 0.0238\n",
      "Ep 027 | beta=0.233 | train R 0.5905 KL 1.0965 P 0.0186 | val R 0.5590 KL 1.2559 P 0.0226\n",
      "Ep 028 | beta=0.242 | train R 0.5576 KL 1.0139 P 0.0176 | val R 0.5687 KL 1.2308 P 0.0284\n",
      "Ep 029 | beta=0.250 | train R 0.5721 KL 1.0723 P 0.0204 | val R 0.6749 KL 1.1191 P 0.0205\n",
      "Ep 030 | beta=0.258 | train R 0.6112 KL 1.0450 P 0.0208 | val R 0.6288 KL 1.1895 P 0.0179\n",
      "Ep 031 | beta=0.267 | train R 0.5782 KL 1.0848 P 0.0183 | val R 0.6672 KL 1.0043 P 0.0253\n",
      "Ep 032 | beta=0.275 | train R 0.6148 KL 0.8237 P 0.0201 | val R 0.6097 KL 0.8548 P 0.0274\n",
      "Ep 033 | beta=0.283 | train R 0.6486 KL 0.8249 P 0.0187 | val R 0.6360 KL 0.9744 P 0.0278\n",
      "Ep 034 | beta=0.292 | train R 0.6266 KL 0.8808 P 0.0190 | val R 0.6140 KL 0.9598 P 0.0246\n",
      "Ep 035 | beta=0.300 | train R 0.6228 KL 0.8302 P 0.0195 | val R 0.6530 KL 0.9147 P 0.0218\n",
      "Ep 036 | beta=0.308 | train R 0.6049 KL 0.8273 P 0.0200 | val R 0.6084 KL 0.9414 P 0.0244\n",
      "Ep 037 | beta=0.317 | train R 0.6890 KL 0.7291 P 0.0207 | val R 0.6751 KL 0.9172 P 0.0208\n",
      "Ep 038 | beta=0.325 | train R 0.6429 KL 0.7256 P 0.0209 | val R 0.6846 KL 0.8338 P 0.0235\n",
      "Ep 039 | beta=0.333 | train R 0.6809 KL 0.6193 P 0.0180 | val R 0.6499 KL 0.7894 P 0.0227\n",
      "Ep 040 | beta=0.342 | train R 0.6844 KL 0.6746 P 0.0198 | val R 0.5997 KL 0.8283 P 0.0219\n",
      "Ep 041 | beta=0.350 | train R 0.7002 KL 0.6414 P 0.0178 | val R 0.6643 KL 0.7167 P 0.0212\n",
      "Ep 042 | beta=0.358 | train R 0.7062 KL 0.5458 P 0.0197 | val R 0.7542 KL 0.6361 P 0.0194\n",
      "Ep 043 | beta=0.367 | train R 0.6867 KL 0.5284 P 0.0179 | val R 0.6906 KL 0.6455 P 0.0205\n",
      "Ep 044 | beta=0.375 | train R 0.6655 KL 0.4939 P 0.0203 | val R 0.7367 KL 0.5754 P 0.0187\n",
      "Ep 045 | beta=0.383 | train R 0.7850 KL 0.4608 P 0.0223 | val R 0.6732 KL 0.5888 P 0.0241\n",
      "Ep 046 | beta=0.392 | train R 0.7407 KL 0.4651 P 0.0216 | val R 0.6655 KL 0.5928 P 0.0185\n",
      "Ep 047 | beta=0.400 | train R 0.7791 KL 0.4084 P 0.0226 | val R 0.8254 KL 0.5839 P 0.0258\n",
      "Ep 048 | beta=0.408 | train R 0.7647 KL 0.4585 P 0.0211 | val R 0.7557 KL 0.6366 P 0.0189\n",
      "Ep 049 | beta=0.417 | train R 0.7539 KL 0.4717 P 0.0194 | val R 0.7045 KL 0.6482 P 0.0214\n",
      "Ep 050 | beta=0.425 | train R 0.7341 KL 0.4220 P 0.0195 | val R 0.6927 KL 0.5850 P 0.0224\n",
      "Ep 051 | beta=0.433 | train R 0.7620 KL 0.4320 P 0.0198 | val R 0.7911 KL 0.5863 P 0.0184\n",
      "Ep 052 | beta=0.442 | train R 0.7266 KL 0.3720 P 0.0200 | val R 0.8004 KL 0.4406 P 0.0252\n",
      "Ep 053 | beta=0.450 | train R 0.7465 KL 0.3181 P 0.0197 | val R 0.7379 KL 0.4526 P 0.0186\n",
      "Ep 054 | beta=0.458 | train R 0.7725 KL 0.3488 P 0.0218 | val R 0.7852 KL 0.5421 P 0.0231\n",
      "Ep 055 | beta=0.467 | train R 0.7966 KL 0.3588 P 0.0198 | val R 0.7919 KL 0.5405 P 0.0203\n",
      "Ep 056 | beta=0.475 | train R 0.7847 KL 0.3342 P 0.0211 | val R 0.7885 KL 0.4208 P 0.0248\n",
      "Ep 057 | beta=0.483 | train R 0.7892 KL 0.2864 P 0.0199 | val R 0.7839 KL 0.4151 P 0.0181\n",
      "Ep 058 | beta=0.492 | train R 0.8542 KL 0.3149 P 0.0231 | val R 0.8008 KL 0.4861 P 0.0216\n",
      "Ep 059 | beta=0.500 | train R 0.7944 KL 0.3067 P 0.0210 | val R 0.8295 KL 0.4603 P 0.0209\n",
      "Ep 060 | beta=0.508 | train R 0.7943 KL 0.3020 P 0.0209 | val R 0.8628 KL 0.4795 P 0.0210\n",
      "Ep 061 | beta=0.517 | train R 0.8315 KL 0.3096 P 0.0222 | val R 0.7850 KL 0.4944 P 0.0250\n",
      "Ep 062 | beta=0.525 | train R 0.8023 KL 0.2927 P 0.0196 | val R 0.7556 KL 0.4007 P 0.0263\n",
      "Ep 063 | beta=0.533 | train R 0.8358 KL 0.2491 P 0.0206 | val R 0.8095 KL 0.4249 P 0.0175\n",
      "Ep 064 | beta=0.542 | train R 0.8157 KL 0.2533 P 0.0213 | val R 0.7603 KL 0.4356 P 0.0189\n",
      "Ep 065 | beta=0.550 | train R 0.8399 KL 0.2438 P 0.0193 | val R 0.8159 KL 0.3936 P 0.0223\n",
      "Ep 066 | beta=0.558 | train R 0.8169 KL 0.2273 P 0.0213 | val R 0.8154 KL 0.3601 P 0.0213\n",
      "Ep 067 | beta=0.567 | train R 0.8611 KL 0.2370 P 0.0200 | val R 0.9236 KL 0.3708 P 0.0194\n",
      "Ep 068 | beta=0.575 | train R 0.8532 KL 0.2475 P 0.0231 | val R 0.7984 KL 0.4112 P 0.0214\n",
      "Ep 069 | beta=0.583 | train R 0.8358 KL 0.2225 P 0.0210 | val R 0.8242 KL 0.3487 P 0.0203\n",
      "Ep 070 | beta=0.592 | train R 0.8277 KL 0.1826 P 0.0208 | val R 0.8076 KL 0.3254 P 0.0212\n",
      "Ep 071 | beta=0.600 | train R 0.8711 KL 0.1806 P 0.0187 | val R 0.8156 KL 0.3433 P 0.0215\n",
      "Ep 072 | beta=0.608 | train R 0.8044 KL 0.1913 P 0.0225 | val R 0.8686 KL 0.3508 P 0.0211\n",
      "Ep 073 | beta=0.617 | train R 0.8709 KL 0.1645 P 0.0204 | val R 0.7813 KL 0.3386 P 0.0196\n",
      "Ep 074 | beta=0.625 | train R 0.8454 KL 0.2055 P 0.0200 | val R 0.7805 KL 0.3716 P 0.0224\n",
      "Ep 075 | beta=0.633 | train R 0.8229 KL 0.2064 P 0.0190 | val R 0.9048 KL 0.2813 P 0.0222\n",
      "Ep 076 | beta=0.642 | train R 0.8863 KL 0.1669 P 0.0195 | val R 0.8877 KL 0.2807 P 0.0232\n",
      "Ep 077 | beta=0.650 | train R 0.8701 KL 0.1664 P 0.0201 | val R 0.9115 KL 0.2761 P 0.0188\n",
      "Ep 078 | beta=0.658 | train R 0.8841 KL 0.1359 P 0.0213 | val R 0.9958 KL 0.2130 P 0.0214\n",
      "Ep 079 | beta=0.667 | train R 0.8929 KL 0.1200 P 0.0206 | val R 0.8534 KL 0.2514 P 0.0202\n",
      "Ep 080 | beta=0.675 | train R 0.8607 KL 0.1370 P 0.0199 | val R 0.7387 KL 0.2755 P 0.0261\n",
      "Ep 081 | beta=0.683 | train R 0.8980 KL 0.1175 P 0.0216 | val R 0.8766 KL 0.2091 P 0.0230\n",
      "Ep 082 | beta=0.692 | train R 0.9247 KL 0.0937 P 0.0205 | val R 0.9716 KL 0.2043 P 0.0193\n",
      "Ep 083 | beta=0.700 | train R 0.9172 KL 0.0849 P 0.0199 | val R 1.0258 KL 0.2012 P 0.0212\n",
      "Ep 084 | beta=0.708 | train R 0.9077 KL 0.0998 P 0.0197 | val R 0.9670 KL 0.1983 P 0.0190\n",
      "Ep 085 | beta=0.717 | train R 0.9064 KL 0.0818 P 0.0215 | val R 0.9067 KL 0.1783 P 0.0206\n",
      "Ep 086 | beta=0.725 | train R 0.8798 KL 0.0773 P 0.0188 | val R 0.9248 KL 0.1742 P 0.0222\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
