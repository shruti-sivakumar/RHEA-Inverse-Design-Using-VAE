{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, joblib, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvae import CVAE, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54415a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, device, epoch, warmup_epochs=120, beta_max=1.0, lambda_prop=1.0):\n",
    "    model.train()\n",
    "    if epoch < warmup_epochs:\n",
    "        beta = beta_max * (epoch+1) / warmup_epochs\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, y_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl    = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop  = nn.functional.mse_loss(y_pred, yb, reduction=\"mean\")\n",
    "\n",
    "        loss = recon + beta*kl + lambda_prop*prop\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        recon_losses.append(recon.item())\n",
    "        kl_losses.append(kl.item())\n",
    "        prop_losses.append(prop.item())\n",
    "\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses), beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, lambda_prop=1.0):\n",
    "    model.eval()\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, y_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl    = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop  = nn.functional.mse_loss(y_pred, yb, reduction=\"mean\")\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item()); prop_losses.append(prop.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c316473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 000 | beta=0.008 | train R 0.9515 KL 0.3671 P 0.1742 | val R 1.7081 KL 0.6130 P 0.0542\n",
      "Ep 001 | beta=0.017 | train R 1.5597 KL 0.8637 P 0.0544 | val R 1.6844 KL 0.9779 P 0.0192\n",
      "Ep 002 | beta=0.025 | train R 1.5184 KL 1.2003 P 0.0414 | val R 1.6218 KL 1.0488 P 0.0293\n",
      "Ep 003 | beta=0.033 | train R 0.7947 KL 1.7120 P 0.0381 | val R 1.6676 KL 1.3464 P 0.0358\n",
      "Ep 004 | beta=0.042 | train R 0.6625 KL 1.7029 P 0.0373 | val R 1.6152 KL 1.2205 P 0.0273\n",
      "Ep 005 | beta=0.050 | train R 0.9143 KL 1.8811 P 0.0338 | val R 1.5870 KL 1.2749 P 0.0341\n",
      "Ep 006 | beta=0.058 | train R 0.6377 KL 1.6419 P 0.0316 | val R 1.6057 KL 1.4838 P 0.0307\n",
      "Ep 007 | beta=0.067 | train R 0.5295 KL 2.1282 P 0.0317 | val R 1.5707 KL 1.7599 P 0.0213\n",
      "Ep 008 | beta=0.075 | train R 0.5251 KL 2.3717 P 0.0227 | val R 1.5178 KL 1.9200 P 0.0203\n",
      "Ep 009 | beta=0.083 | train R 0.4903 KL 2.3777 P 0.0247 | val R 1.4726 KL 1.8635 P 0.0318\n",
      "Ep 010 | beta=0.092 | train R 0.4249 KL 2.3038 P 0.0211 | val R 1.4766 KL 1.6755 P 0.0280\n",
      "Ep 011 | beta=0.100 | train R 0.4388 KL 2.3110 P 0.0271 | val R 1.4342 KL 1.5328 P 0.0261\n",
      "Ep 012 | beta=0.108 | train R 0.4767 KL 2.0274 P 0.0243 | val R 1.4288 KL 1.6023 P 0.0253\n",
      "Ep 013 | beta=0.117 | train R 0.4076 KL 1.9559 P 0.0254 | val R 1.4481 KL 1.6284 P 0.0254\n",
      "Ep 014 | beta=0.125 | train R 0.4619 KL 2.0341 P 0.0234 | val R 1.3386 KL 1.6386 P 0.0319\n",
      "Ep 015 | beta=0.133 | train R 0.3871 KL 2.0271 P 0.0261 | val R 1.3517 KL 1.6046 P 0.0298\n",
      "Ep 016 | beta=0.142 | train R 0.3506 KL 2.0251 P 0.0227 | val R 1.3618 KL 1.5060 P 0.0220\n",
      "Ep 017 | beta=0.150 | train R 0.5026 KL 1.9099 P 0.0217 | val R 1.2667 KL 1.5213 P 0.0294\n",
      "Ep 018 | beta=0.158 | train R 0.3716 KL 1.7080 P 0.0233 | val R 1.2477 KL 1.5362 P 0.0260\n",
      "Ep 019 | beta=0.167 | train R 0.4422 KL 1.7860 P 0.0203 | val R 1.1238 KL 1.3891 P 0.0277\n",
      "Ep 020 | beta=0.175 | train R 0.4591 KL 1.9977 P 0.0224 | val R 1.3559 KL 1.3851 P 0.0209\n",
      "Ep 021 | beta=0.183 | train R 0.5088 KL 2.4597 P 0.0251 | val R 1.3033 KL 1.3500 P 0.0299\n",
      "Ep 022 | beta=0.192 | train R 0.4002 KL 1.5178 P 0.0214 | val R 1.4374 KL 1.2333 P 0.0179\n",
      "Ep 023 | beta=0.200 | train R 0.4967 KL 1.4087 P 0.0229 | val R 1.3025 KL 1.1276 P 0.0288\n",
      "Ep 024 | beta=0.208 | train R 0.4517 KL 1.4302 P 0.0205 | val R 1.2214 KL 1.1202 P 0.0300\n",
      "Ep 025 | beta=0.217 | train R 0.5051 KL 1.4198 P 0.0200 | val R 1.1847 KL 1.1615 P 0.0241\n",
      "Ep 026 | beta=0.225 | train R 0.4775 KL 1.5745 P 0.0219 | val R 1.1547 KL 1.2084 P 0.0223\n",
      "Ep 027 | beta=0.233 | train R 0.4887 KL 1.5262 P 0.0267 | val R 1.0038 KL 1.1490 P 0.0226\n",
      "Ep 028 | beta=0.242 | train R 0.4685 KL 1.4634 P 0.0153 | val R 1.1915 KL 1.1736 P 0.0245\n",
      "Ep 029 | beta=0.250 | train R 0.4659 KL 1.6016 P 0.0250 | val R 1.3442 KL 1.1165 P 0.0221\n",
      "Ep 030 | beta=0.258 | train R 0.4759 KL 1.2550 P 0.0160 | val R 1.0947 KL 0.9903 P 0.0269\n",
      "Ep 031 | beta=0.267 | train R 0.4666 KL 1.2552 P 0.0193 | val R 1.1345 KL 0.8041 P 0.0247\n",
      "Ep 032 | beta=0.275 | train R 0.5121 KL 0.9373 P 0.0252 | val R 1.5460 KL 0.7118 P 0.0242\n",
      "Ep 033 | beta=0.283 | train R 0.5274 KL 0.8633 P 0.0274 | val R 1.2404 KL 0.6505 P 0.0243\n",
      "Ep 034 | beta=0.292 | train R 0.5832 KL 0.9075 P 0.0189 | val R 0.9729 KL 0.6676 P 0.0264\n",
      "Ep 035 | beta=0.300 | train R 0.4979 KL 0.8724 P 0.0211 | val R 1.5082 KL 0.6396 P 0.0311\n",
      "Ep 036 | beta=0.308 | train R 0.6102 KL 1.0978 P 0.0203 | val R 1.0325 KL 0.6725 P 0.0199\n",
      "Ep 037 | beta=0.317 | train R 0.5590 KL 0.9182 P 0.0155 | val R 1.3750 KL 0.7061 P 0.0213\n",
      "Ep 038 | beta=0.325 | train R 0.5603 KL 0.8786 P 0.0231 | val R 0.9211 KL 0.6560 P 0.0201\n",
      "Ep 039 | beta=0.333 | train R 0.4669 KL 0.9486 P 0.0189 | val R 1.5162 KL 0.5850 P 0.0260\n",
      "Ep 040 | beta=0.342 | train R 0.6402 KL 0.7459 P 0.0181 | val R 1.2964 KL 0.6002 P 0.0261\n",
      "Ep 041 | beta=0.350 | train R 0.6251 KL 0.7170 P 0.0147 | val R 1.2539 KL 0.5755 P 0.0199\n",
      "Ep 042 | beta=0.358 | train R 0.5657 KL 0.6889 P 0.0177 | val R 1.2454 KL 0.5043 P 0.0207\n",
      "Ep 043 | beta=0.367 | train R 1.0952 KL 1.4095 P 0.0213 | val R 1.1769 KL 0.4921 P 0.0165\n",
      "Ep 044 | beta=0.375 | train R 0.6159 KL 0.7339 P 0.0173 | val R 1.2250 KL 0.5707 P 0.0187\n",
      "Ep 045 | beta=0.383 | train R 0.6330 KL 0.7248 P 0.0185 | val R 1.5072 KL 0.5916 P 0.0263\n",
      "Ep 046 | beta=0.392 | train R 0.5768 KL 0.6672 P 0.0252 | val R 1.2516 KL 0.5683 P 0.0224\n",
      "Ep 047 | beta=0.400 | train R 0.7404 KL 0.6290 P 0.0151 | val R 1.3023 KL 0.4794 P 0.0222\n",
      "Ep 048 | beta=0.408 | train R 0.6638 KL 0.6450 P 0.0189 | val R 1.2364 KL 0.4519 P 0.0147\n",
      "Ep 049 | beta=0.417 | train R 0.6548 KL 0.5680 P 0.0130 | val R 1.2825 KL 0.4324 P 0.0260\n",
      "Ep 050 | beta=0.425 | train R 0.6988 KL 0.5228 P 0.0222 | val R 1.5031 KL 0.4374 P 0.0210\n",
      "Ep 051 | beta=0.433 | train R 0.6349 KL 0.4952 P 0.0176 | val R 1.0846 KL 0.4081 P 0.0232\n",
      "Ep 052 | beta=0.442 | train R 0.7697 KL 0.9389 P 0.0142 | val R 1.5999 KL 0.3698 P 0.0223\n",
      "Ep 053 | beta=0.450 | train R 0.6681 KL 1.1417 P 0.0146 | val R 1.1542 KL 0.4226 P 0.0168\n",
      "Ep 054 | beta=0.458 | train R 0.6362 KL 0.5584 P 0.0163 | val R 1.0659 KL 0.4158 P 0.0153\n",
      "Ep 055 | beta=0.467 | train R 0.6156 KL 0.4240 P 0.0144 | val R 1.4573 KL 0.3748 P 0.0193\n",
      "Ep 056 | beta=0.475 | train R 0.6881 KL 0.3911 P 0.0172 | val R 1.3320 KL 0.3708 P 0.0155\n",
      "Ep 057 | beta=0.483 | train R 0.6786 KL 0.4329 P 0.0143 | val R 1.0678 KL 0.3984 P 0.0164\n",
      "Ep 058 | beta=0.492 | train R 0.8344 KL 0.5446 P 0.0150 | val R 0.9563 KL 0.4679 P 0.0189\n",
      "Ep 059 | beta=0.500 | train R 0.6483 KL 0.5000 P 0.0142 | val R 1.0481 KL 0.4644 P 0.0174\n",
      "Ep 060 | beta=0.508 | train R 0.6144 KL 0.5727 P 0.0096 | val R 1.2009 KL 0.4025 P 0.0095\n",
      "Ep 061 | beta=0.517 | train R 0.6753 KL 0.3803 P 0.0127 | val R 1.2800 KL 0.3397 P 0.0148\n",
      "Ep 062 | beta=0.525 | train R 0.7856 KL 0.3809 P 0.0109 | val R 1.4450 KL 0.3388 P 0.0135\n",
      "Ep 063 | beta=0.533 | train R 0.7938 KL 0.5177 P 0.0116 | val R 1.2778 KL 0.3106 P 0.0184\n",
      "Ep 064 | beta=0.542 | train R 0.7309 KL 0.3406 P 0.0128 | val R 1.1394 KL 0.3263 P 0.0109\n",
      "Ep 065 | beta=0.550 | train R 0.8157 KL 0.3981 P 0.0112 | val R 1.4034 KL 0.3209 P 0.0098\n",
      "Ep 066 | beta=0.558 | train R 0.7510 KL 0.3139 P 0.0121 | val R 1.0917 KL 0.3306 P 0.0127\n",
      "Ep 067 | beta=0.567 | train R 0.6954 KL 0.3216 P 0.0123 | val R 1.3257 KL 0.3683 P 0.0175\n",
      "Ep 068 | beta=0.575 | train R 1.0395 KL 0.4106 P 0.0231 | val R 1.4267 KL 0.3349 P 0.0104\n",
      "Ep 069 | beta=0.583 | train R 0.6512 KL 0.2686 P 0.0121 | val R 1.2594 KL 0.3150 P 0.0085\n",
      "Ep 070 | beta=0.592 | train R 1.0682 KL 0.4302 P 0.0132 | val R 1.4366 KL 0.2946 P 0.0134\n",
      "Ep 071 | beta=0.600 | train R 0.7360 KL 0.2333 P 0.0148 | val R 1.4582 KL 0.2694 P 0.0157\n",
      "Ep 072 | beta=0.608 | train R 0.7040 KL 0.2063 P 0.0127 | val R 1.2749 KL 0.2598 P 0.0197\n",
      "Ep 073 | beta=0.617 | train R 0.7989 KL 0.2396 P 0.0117 | val R 1.7247 KL 0.2387 P 0.0111\n",
      "Ep 074 | beta=0.625 | train R 0.7231 KL 0.1875 P 0.0184 | val R 1.3785 KL 0.2282 P 0.0100\n",
      "Ep 075 | beta=0.633 | train R 0.7878 KL 0.2284 P 0.0091 | val R 1.5679 KL 0.2644 P 0.0157\n",
      "Ep 076 | beta=0.642 | train R 0.7359 KL 0.2184 P 0.0092 | val R 1.4196 KL 0.3154 P 0.0130\n",
      "Ep 077 | beta=0.650 | train R 0.6857 KL 0.2367 P 0.0102 | val R 1.3269 KL 0.2885 P 0.0092\n",
      "Ep 078 | beta=0.658 | train R 0.6474 KL 0.2734 P 0.0116 | val R 1.1890 KL 0.2790 P 0.0115\n",
      "Ep 079 | beta=0.667 | train R 0.7072 KL 0.5012 P 0.0090 | val R 1.3827 KL 0.3026 P 0.0097\n",
      "Ep 080 | beta=0.675 | train R 0.7011 KL 0.2644 P 0.0116 | val R 1.4069 KL 0.2652 P 0.0168\n",
      "Ep 081 | beta=0.683 | train R 0.7666 KL 0.1945 P 0.0137 | val R 1.4448 KL 0.2459 P 0.0206\n",
      "Ep 082 | beta=0.692 | train R 0.8649 KL 0.1974 P 0.0134 | val R 0.9719 KL 0.2322 P 0.0081\n",
      "Ep 083 | beta=0.700 | train R 0.7476 KL 0.1447 P 0.0131 | val R 1.4566 KL 0.2418 P 0.0133\n",
      "Ep 084 | beta=0.708 | train R 0.7418 KL 0.1532 P 0.0067 | val R 1.2506 KL 0.2053 P 0.0109\n",
      "Ep 085 | beta=0.717 | train R 0.7795 KL 0.1353 P 0.0110 | val R 1.1912 KL 0.2455 P 0.0106\n",
      "Ep 086 | beta=0.725 | train R 0.9217 KL 0.1547 P 0.0113 | val R 1.3821 KL 0.2357 P 0.0099\n",
      "Ep 087 | beta=0.733 | train R 0.7229 KL 0.1632 P 0.0086 | val R 1.5157 KL 0.2184 P 0.0115\n",
      "Ep 088 | beta=0.742 | train R 0.8874 KL 0.2107 P 0.0114 | val R 1.2628 KL 0.2119 P 0.0126\n",
      "Ep 089 | beta=0.750 | train R 0.7464 KL 0.1268 P 0.0084 | val R 1.3574 KL 0.2033 P 0.0113\n",
      "Ep 090 | beta=0.758 | train R 0.7284 KL 0.1332 P 0.0120 | val R 1.3975 KL 0.2057 P 0.0101\n",
      "Ep 091 | beta=0.767 | train R 0.7606 KL 0.1348 P 0.0114 | val R 1.2265 KL 0.2177 P 0.0121\n",
      "Ep 092 | beta=0.775 | train R 0.8163 KL 0.1376 P 0.0098 | val R 1.3109 KL 0.2174 P 0.0081\n",
      "Ep 093 | beta=0.783 | train R 0.6577 KL 0.1356 P 0.0118 | val R 1.1871 KL 0.1823 P 0.0086\n",
      "Ep 094 | beta=0.792 | train R 0.7348 KL 0.1087 P 0.0085 | val R 1.0979 KL 0.1747 P 0.0120\n",
      "Ep 095 | beta=0.800 | train R 0.8068 KL 0.0923 P 0.0117 | val R 1.7249 KL 0.1845 P 0.0169\n",
      "Ep 096 | beta=0.808 | train R 0.7996 KL 0.1054 P 0.0087 | val R 0.9889 KL 0.2075 P 0.0089\n",
      "Ep 097 | beta=0.817 | train R 0.7687 KL 0.0996 P 0.0088 | val R 1.2883 KL 0.2034 P 0.0073\n",
      "Ep 098 | beta=0.825 | train R 0.7078 KL 0.0987 P 0.0095 | val R 1.2481 KL 0.2264 P 0.0139\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = np.load(\"../data/processed/data_splits.npz\")  # <- paths per your repo\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "    x_dim, y_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "    train_loader = DataLoader(TabDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "    val_loader   = DataLoader(TabDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "    model = CVAE(x_dim=x_dim, y_dim=y_dim, z_dim=4, hidden=128).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_val = 1e9; patience, bad = 60, 0\n",
    "    EPOCHS = 200\n",
    "    for epoch in range(EPOCHS):\n",
    "        tr_recon, tr_kl, tr_prop, beta = train_epoch(model, train_loader, opt, device,\n",
    "                                                     epoch, warmup_epochs=120, beta_max=1.0, lambda_prop=1.0)\n",
    "        va_recon, va_kl, va_prop = eval_epoch(model, val_loader, device, lambda_prop=1.0)\n",
    "        print(f\"Ep {epoch:03d} | beta={beta:.3f} | \"\n",
    "              f\"train R {tr_recon:.4f} KL {tr_kl:.4f} P {tr_prop:.4f} | \"\n",
    "              f\"val R {va_recon:.4f} KL {va_kl:.4f} P {va_prop:.4f}\")\n",
    "\n",
    "        # Early stop on (recon + prop) to respect both\n",
    "        val_score = va_recon + va_prop\n",
    "        if val_score < best_val - 1e-5:\n",
    "            best_val = val_score; bad = 0\n",
    "            torch.save(model.state_dict(), \"../models/cvae_best.pt\")\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\"); break\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
