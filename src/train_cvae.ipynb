{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, joblib, torch, torch.nn as nn, torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvae import CVAE, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y_cond, y_prop):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cond = torch.tensor(y_cond, dtype=torch.float32)\n",
    "        self.y_prop = torch.tensor(y_prop, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y_cond[i], self.y_prop[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54415a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, device, epoch, warmup_epochs=100, beta_max=2.0, lambda_prop=0.5):\n",
    "    model.train()\n",
    "    beta = beta_max * min(1.0, (epoch + 1) / warmup_epochs)\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, ycb, ypb in loader:\n",
    "        Xb, ycb, ypb = Xb.to(device), ycb.to(device), ypb.to(device)\n",
    "        X_hat, y_prop_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, ycb)\n",
    "        recon = F.mse_loss(X_hat, Xb)\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop = F.mse_loss(y_prop_pred, ypb)\n",
    "        loss = recon + beta * kl + lambda_prop * prop\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item()); prop_losses.append(prop.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses), beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, lambda_prop=0.5):\n",
    "    model.eval()\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, ycb, ypb in loader:\n",
    "        Xb, ycb, ypb = Xb.to(device), ycb.to(device), ypb.to(device)\n",
    "        X_hat, y_prop_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, ycb)\n",
    "        recon = F.mse_loss(X_hat, Xb)\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop = F.mse_loss(y_prop_pred, ypb)\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item()); prop_losses.append(prop.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c316473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = np.load(\"../data/processed/data_splits.npz\")\n",
    "    X_train, X_val = data[\"X_train\"], data[\"X_val\"]\n",
    "    y_cond_train, y_cond_val = data[\"y_cond_train\"], data[\"y_cond_val\"]\n",
    "    y_prop_train, y_prop_val = data[\"y_prop_train\"], data[\"y_prop_val\"]\n",
    "    train_loader = DataLoader(TabDataset(X_train, y_cond_train, y_prop_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TabDataset(X_val, y_cond_val, y_prop_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = CVAE(x_dim=X_train.shape[1], y_cond_dim=y_cond_train.shape[1], y_prop_dim=y_prop_train.shape[1],\n",
    "                 z_dim=8, hidden=256).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_val, patience, bad = 1e9, 40, 0\n",
    "    for epoch in range(200):\n",
    "        tr_recon, tr_kl, tr_prop, beta = train_epoch(model, train_loader, opt, device, epoch)\n",
    "        va_recon, va_kl, va_prop = eval_epoch(model, val_loader, device)\n",
    "        val_score = va_recon + va_prop\n",
    "        print(f\"Ep {epoch:03d} | β={beta:.2f} | Train R {tr_recon:.4f} KL {tr_kl:.4f} P {tr_prop:.4f} | Val R {va_recon:.4f} KL {va_kl:.4f} P {va_prop:.4f}\")\n",
    "        if val_score < best_val - 1e-5:\n",
    "            best_val, bad = val_score, 0\n",
    "            torch.save(model.state_dict(), \"../models/cvae_best.pt\")\n",
    "        else: bad += 1\n",
    "        if bad >= patience: print(\"Early stopping.\"); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8842866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 000 | β=0.02 | Train R 0.9504 KL 0.7096 P 0.1779 | Val R 0.9813 KL 1.8434 P 0.1073\n",
      "Ep 001 | β=0.04 | Train R 0.8500 KL 2.1829 P 0.0734 | Val R 0.8610 KL 2.0003 P 0.0664\n",
      "Ep 002 | β=0.06 | Train R 0.7448 KL 1.8606 P 0.0531 | Val R 0.8342 KL 1.5353 P 0.0548\n",
      "Ep 003 | β=0.08 | Train R 0.7444 KL 1.8643 P 0.0400 | Val R 0.7754 KL 2.0141 P 0.0571\n",
      "Ep 004 | β=0.10 | Train R 0.6658 KL 1.8981 P 0.0427 | Val R 0.7798 KL 1.7623 P 0.0383\n",
      "Ep 005 | β=0.12 | Train R 0.6861 KL 1.7717 P 0.0436 | Val R 0.7310 KL 1.7701 P 0.0505\n",
      "Ep 006 | β=0.14 | Train R 0.6452 KL 1.6984 P 0.0382 | Val R 0.7512 KL 1.4545 P 0.0434\n",
      "Ep 007 | β=0.16 | Train R 0.6863 KL 1.2853 P 0.0379 | Val R 0.7853 KL 1.3367 P 0.0534\n",
      "Ep 008 | β=0.18 | Train R 0.7066 KL 1.2374 P 0.0365 | Val R 0.7493 KL 1.2271 P 0.0432\n",
      "Ep 009 | β=0.20 | Train R 0.6805 KL 1.1107 P 0.0365 | Val R 0.7703 KL 1.2199 P 0.0386\n",
      "Ep 010 | β=0.22 | Train R 0.7082 KL 1.0150 P 0.0328 | Val R 0.6846 KL 1.1076 P 0.0282\n",
      "Ep 011 | β=0.24 | Train R 0.7087 KL 0.9080 P 0.0328 | Val R 0.7220 KL 1.0248 P 0.0476\n",
      "Ep 012 | β=0.26 | Train R 0.7132 KL 0.7617 P 0.0310 | Val R 0.7324 KL 0.8313 P 0.0415\n",
      "Ep 013 | β=0.28 | Train R 0.7191 KL 0.6591 P 0.0359 | Val R 0.7385 KL 0.8385 P 0.0475\n",
      "Ep 014 | β=0.30 | Train R 0.7631 KL 0.6687 P 0.0353 | Val R 0.7491 KL 0.9163 P 0.0363\n",
      "Ep 015 | β=0.32 | Train R 0.7126 KL 0.6118 P 0.0328 | Val R 0.7738 KL 0.6814 P 0.0422\n",
      "Ep 016 | β=0.34 | Train R 0.8139 KL 0.4720 P 0.0329 | Val R 0.7727 KL 0.6527 P 0.0401\n",
      "Ep 017 | β=0.36 | Train R 0.7565 KL 0.4607 P 0.0281 | Val R 0.7555 KL 0.5784 P 0.0383\n",
      "Ep 018 | β=0.38 | Train R 0.7629 KL 0.3872 P 0.0340 | Val R 0.8364 KL 0.4770 P 0.0402\n",
      "Ep 019 | β=0.40 | Train R 0.7904 KL 0.3630 P 0.0304 | Val R 0.9335 KL 0.4967 P 0.0395\n",
      "Ep 020 | β=0.42 | Train R 0.7884 KL 0.3591 P 0.0324 | Val R 0.8117 KL 0.4628 P 0.0385\n",
      "Ep 021 | β=0.44 | Train R 0.8183 KL 0.3296 P 0.0311 | Val R 0.8405 KL 0.4218 P 0.0347\n",
      "Ep 022 | β=0.46 | Train R 0.8315 KL 0.2509 P 0.0284 | Val R 0.9259 KL 0.3653 P 0.0376\n",
      "Ep 023 | β=0.48 | Train R 0.8848 KL 0.2092 P 0.0294 | Val R 0.9352 KL 0.3411 P 0.0354\n",
      "Ep 024 | β=0.50 | Train R 0.8348 KL 0.2151 P 0.0313 | Val R 0.8381 KL 0.2796 P 0.0346\n",
      "Ep 025 | β=0.52 | Train R 0.8522 KL 0.1695 P 0.0304 | Val R 0.9494 KL 0.2102 P 0.0396\n",
      "Ep 026 | β=0.54 | Train R 0.8824 KL 0.1376 P 0.0316 | Val R 0.9561 KL 0.2467 P 0.0308\n",
      "Ep 027 | β=0.56 | Train R 0.8925 KL 0.1590 P 0.0310 | Val R 0.9596 KL 0.2571 P 0.0373\n",
      "Ep 028 | β=0.58 | Train R 0.9025 KL 0.1584 P 0.0306 | Val R 1.0110 KL 0.2537 P 0.0347\n",
      "Ep 029 | β=0.60 | Train R 0.8785 KL 0.1428 P 0.0278 | Val R 0.8413 KL 0.2417 P 0.0316\n",
      "Ep 030 | β=0.62 | Train R 0.8624 KL 0.1378 P 0.0312 | Val R 0.8618 KL 0.2025 P 0.0326\n",
      "Ep 031 | β=0.64 | Train R 0.9186 KL 0.1007 P 0.0286 | Val R 0.8655 KL 0.2064 P 0.0284\n",
      "Ep 032 | β=0.66 | Train R 0.8959 KL 0.1016 P 0.0280 | Val R 0.9239 KL 0.1599 P 0.0262\n",
      "Ep 033 | β=0.68 | Train R 0.9320 KL 0.0991 P 0.0335 | Val R 0.9804 KL 0.1298 P 0.0390\n",
      "Ep 034 | β=0.70 | Train R 0.9469 KL 0.0753 P 0.0330 | Val R 0.9540 KL 0.1088 P 0.0362\n",
      "Ep 035 | β=0.72 | Train R 0.9221 KL 0.0585 P 0.0287 | Val R 1.0165 KL 0.0664 P 0.0394\n",
      "Ep 036 | β=0.74 | Train R 1.0052 KL 0.0478 P 0.0331 | Val R 0.9687 KL 0.0664 P 0.0317\n",
      "Ep 037 | β=0.76 | Train R 0.9228 KL 0.0407 P 0.0322 | Val R 1.0126 KL 0.0843 P 0.0321\n",
      "Ep 038 | β=0.78 | Train R 0.9534 KL 0.0380 P 0.0337 | Val R 1.0142 KL 0.0526 P 0.0348\n",
      "Ep 039 | β=0.80 | Train R 0.9362 KL 0.0313 P 0.0272 | Val R 1.0402 KL 0.0499 P 0.0343\n",
      "Ep 040 | β=0.82 | Train R 0.9474 KL 0.0298 P 0.0285 | Val R 1.0095 KL 0.0537 P 0.0363\n",
      "Ep 041 | β=0.84 | Train R 0.9766 KL 0.0234 P 0.0344 | Val R 1.0569 KL 0.0248 P 0.0302\n",
      "Ep 042 | β=0.86 | Train R 0.9339 KL 0.0156 P 0.0340 | Val R 1.0534 KL 0.0374 P 0.0383\n",
      "Ep 043 | β=0.88 | Train R 0.9475 KL 0.0140 P 0.0316 | Val R 1.0270 KL 0.0263 P 0.0322\n",
      "Ep 044 | β=0.90 | Train R 0.9628 KL 0.0135 P 0.0331 | Val R 1.0221 KL 0.0257 P 0.0368\n",
      "Ep 045 | β=0.92 | Train R 0.9608 KL 0.0148 P 0.0247 | Val R 1.0234 KL 0.0249 P 0.0314\n",
      "Ep 046 | β=0.94 | Train R 0.9332 KL 0.0100 P 0.0309 | Val R 1.0003 KL 0.0336 P 0.0303\n",
      "Ep 047 | β=0.96 | Train R 0.9617 KL 0.0175 P 0.0302 | Val R 1.0470 KL 0.0225 P 0.0376\n",
      "Ep 048 | β=0.98 | Train R 0.9855 KL 0.0133 P 0.0266 | Val R 1.0608 KL 0.0272 P 0.0274\n",
      "Ep 049 | β=1.00 | Train R 0.9353 KL 0.0114 P 0.0282 | Val R 1.0437 KL 0.0252 P 0.0354\n",
      "Ep 050 | β=1.02 | Train R 0.9248 KL 0.0093 P 0.0257 | Val R 1.0146 KL 0.0175 P 0.0298\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
