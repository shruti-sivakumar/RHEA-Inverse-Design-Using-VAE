{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, joblib, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvae import CVAE, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54415a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, device, epoch, warmup_epochs=120, beta_max=1.0, lambda_prop=1.0):\n",
    "    model.train()\n",
    "    if epoch < warmup_epochs:\n",
    "        beta = beta_max * (epoch+1) / warmup_epochs\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, y_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl    = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop  = nn.functional.mse_loss(y_pred, yb, reduction=\"mean\")\n",
    "\n",
    "        loss = recon + beta*kl + lambda_prop*prop\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        recon_losses.append(recon.item())\n",
    "        kl_losses.append(kl.item())\n",
    "        prop_losses.append(prop.item())\n",
    "\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses), beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, lambda_prop=1.0):\n",
    "    model.eval()\n",
    "    recon_losses, kl_losses, prop_losses = [], [], []\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, y_pred, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl    = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        prop  = nn.functional.mse_loss(y_pred, yb, reduction=\"mean\")\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item()); prop_losses.append(prop.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), np.mean(prop_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c316473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 000 | beta=0.008 | train R 0.9830 KL 0.2507 P 0.1428 | val R 1.3181 KL 0.4729 P 0.0644\n",
      "Ep 001 | beta=0.017 | train R 0.8954 KL 0.6110 P 0.0544 | val R 1.2966 KL 0.5310 P 0.0302\n",
      "Ep 002 | beta=0.025 | train R 0.8768 KL 0.6492 P 0.0367 | val R 1.2750 KL 0.6341 P 0.0259\n",
      "Ep 003 | beta=0.033 | train R 0.8782 KL 1.0380 P 0.0256 | val R 1.2455 KL 0.9645 P 0.0284\n",
      "Ep 004 | beta=0.042 | train R 0.8957 KL 1.6814 P 0.0225 | val R 1.2272 KL 1.1535 P 0.0362\n",
      "Ep 005 | beta=0.050 | train R 0.7506 KL 1.5714 P 0.0207 | val R 1.2274 KL 1.2477 P 0.0191\n",
      "Ep 006 | beta=0.058 | train R 0.7835 KL 1.5979 P 0.0249 | val R 1.1943 KL 1.1999 P 0.0264\n",
      "Ep 007 | beta=0.067 | train R 0.7182 KL 2.0141 P 0.0245 | val R 1.1793 KL 1.2392 P 0.0209\n",
      "Ep 008 | beta=0.075 | train R 0.6798 KL 1.8302 P 0.0295 | val R 1.1526 KL 1.2697 P 0.0233\n",
      "Ep 009 | beta=0.083 | train R 0.6434 KL 1.6847 P 0.0224 | val R 1.1539 KL 1.2098 P 0.0199\n",
      "Ep 010 | beta=0.092 | train R 0.6542 KL 1.5838 P 0.0235 | val R 1.0921 KL 1.2704 P 0.0235\n",
      "Ep 011 | beta=0.100 | train R 0.6105 KL 1.9844 P 0.0229 | val R 1.1447 KL 1.4129 P 0.0195\n",
      "Ep 012 | beta=0.108 | train R 0.6029 KL 1.8586 P 0.0217 | val R 1.0878 KL 1.5560 P 0.0200\n",
      "Ep 013 | beta=0.117 | train R 0.6253 KL 1.7136 P 0.0186 | val R 1.0567 KL 1.4540 P 0.0190\n",
      "Ep 014 | beta=0.125 | train R 0.5781 KL 1.6495 P 0.0251 | val R 1.0350 KL 1.3450 P 0.0244\n",
      "Ep 015 | beta=0.133 | train R 0.5379 KL 1.4599 P 0.0184 | val R 1.0898 KL 1.1461 P 0.0200\n",
      "Ep 016 | beta=0.142 | train R 0.5927 KL 1.6302 P 0.0215 | val R 1.0196 KL 1.1185 P 0.0149\n",
      "Ep 017 | beta=0.150 | train R 0.6396 KL 1.3570 P 0.0261 | val R 1.0462 KL 1.1592 P 0.0210\n",
      "Ep 018 | beta=0.158 | train R 0.5651 KL 1.4555 P 0.0164 | val R 1.1099 KL 1.0254 P 0.0217\n",
      "Ep 019 | beta=0.167 | train R 0.6056 KL 1.4313 P 0.0257 | val R 0.8943 KL 0.9729 P 0.0175\n",
      "Ep 020 | beta=0.175 | train R 0.5956 KL 1.1363 P 0.0222 | val R 0.9530 KL 0.9811 P 0.0141\n",
      "Ep 021 | beta=0.183 | train R 0.5898 KL 1.0025 P 0.0183 | val R 1.0684 KL 0.9034 P 0.0205\n",
      "Ep 022 | beta=0.192 | train R 0.5572 KL 0.9455 P 0.0171 | val R 1.0102 KL 0.7768 P 0.0169\n",
      "Ep 023 | beta=0.200 | train R 0.6449 KL 0.9810 P 0.0147 | val R 1.2062 KL 0.6949 P 0.0192\n",
      "Ep 024 | beta=0.208 | train R 0.6373 KL 0.9974 P 0.0233 | val R 0.9616 KL 0.7672 P 0.0145\n",
      "Ep 025 | beta=0.217 | train R 0.6047 KL 1.0195 P 0.0188 | val R 1.0810 KL 0.7859 P 0.0236\n",
      "Ep 026 | beta=0.225 | train R 0.7967 KL 0.9790 P 0.0154 | val R 1.0406 KL 0.7833 P 0.0185\n",
      "Ep 027 | beta=0.233 | train R 0.6527 KL 0.9586 P 0.0133 | val R 0.9955 KL 0.7560 P 0.0119\n",
      "Ep 028 | beta=0.242 | train R 0.6240 KL 0.8786 P 0.0178 | val R 1.1000 KL 0.7294 P 0.0192\n",
      "Ep 029 | beta=0.250 | train R 0.7059 KL 0.8229 P 0.0161 | val R 1.0386 KL 0.7023 P 0.0174\n",
      "Ep 030 | beta=0.258 | train R 0.7154 KL 1.0328 P 0.0164 | val R 0.9326 KL 0.5952 P 0.0134\n",
      "Ep 031 | beta=0.267 | train R 0.6341 KL 0.7308 P 0.0158 | val R 1.0943 KL 0.5699 P 0.0117\n",
      "Ep 032 | beta=0.275 | train R 0.7509 KL 0.6948 P 0.0121 | val R 1.0546 KL 0.6076 P 0.0136\n",
      "Ep 033 | beta=0.283 | train R 0.6282 KL 0.6791 P 0.0155 | val R 1.0415 KL 0.6125 P 0.0166\n",
      "Ep 034 | beta=0.292 | train R 0.8521 KL 1.0108 P 0.0168 | val R 1.2289 KL 0.6327 P 0.0122\n",
      "Ep 035 | beta=0.300 | train R 0.6541 KL 0.6584 P 0.0127 | val R 1.1945 KL 0.6304 P 0.0111\n",
      "Ep 036 | beta=0.308 | train R 0.6796 KL 0.6424 P 0.0131 | val R 1.0757 KL 0.6057 P 0.0192\n",
      "Ep 037 | beta=0.317 | train R 0.8350 KL 0.6032 P 0.0133 | val R 1.2032 KL 0.6563 P 0.0112\n",
      "Ep 038 | beta=0.325 | train R 0.6902 KL 0.5420 P 0.0124 | val R 1.0441 KL 0.5298 P 0.0099\n",
      "Ep 039 | beta=0.333 | train R 0.7683 KL 0.6058 P 0.0103 | val R 1.1586 KL 0.3866 P 0.0092\n",
      "Ep 040 | beta=0.342 | train R 0.7676 KL 0.6756 P 0.0115 | val R 1.2138 KL 0.4350 P 0.0116\n",
      "Ep 041 | beta=0.350 | train R 0.7438 KL 0.4220 P 0.0153 | val R 1.0236 KL 0.5112 P 0.0097\n",
      "Ep 042 | beta=0.358 | train R 0.7767 KL 0.5745 P 0.0107 | val R 1.1543 KL 0.4711 P 0.0137\n",
      "Ep 043 | beta=0.367 | train R 0.6910 KL 0.4005 P 0.0110 | val R 0.7854 KL 0.3898 P 0.0113\n",
      "Ep 044 | beta=0.375 | train R 0.8023 KL 0.3641 P 0.0135 | val R 1.1387 KL 0.3940 P 0.0113\n",
      "Ep 045 | beta=0.383 | train R 0.7797 KL 0.3438 P 0.0104 | val R 1.1914 KL 0.4354 P 0.0087\n",
      "Ep 046 | beta=0.392 | train R 0.8028 KL 0.3023 P 0.0101 | val R 1.1790 KL 0.4315 P 0.0097\n",
      "Ep 047 | beta=0.400 | train R 0.8228 KL 0.2883 P 0.0113 | val R 1.1489 KL 0.4478 P 0.0119\n",
      "Ep 048 | beta=0.408 | train R 0.7757 KL 0.2914 P 0.0084 | val R 1.1191 KL 0.4169 P 0.0108\n",
      "Ep 049 | beta=0.417 | train R 0.7249 KL 0.2961 P 0.0078 | val R 1.1084 KL 0.4387 P 0.0140\n",
      "Ep 050 | beta=0.425 | train R 0.7485 KL 0.2490 P 0.0102 | val R 1.2242 KL 0.4755 P 0.0146\n",
      "Ep 051 | beta=0.433 | train R 0.8317 KL 0.2701 P 0.0095 | val R 0.9654 KL 0.4784 P 0.0161\n",
      "Ep 052 | beta=0.442 | train R 0.7800 KL 0.2654 P 0.0072 | val R 1.2347 KL 0.4351 P 0.0089\n",
      "Ep 053 | beta=0.450 | train R 0.7438 KL 0.2414 P 0.0103 | val R 0.9472 KL 0.4288 P 0.0096\n",
      "Ep 054 | beta=0.458 | train R 0.9072 KL 0.2388 P 0.0089 | val R 1.2971 KL 0.4516 P 0.0053\n",
      "Ep 055 | beta=0.467 | train R 0.7432 KL 0.2217 P 0.0071 | val R 1.1686 KL 0.3725 P 0.0139\n",
      "Ep 056 | beta=0.475 | train R 0.7764 KL 0.2091 P 0.0116 | val R 1.2591 KL 0.4173 P 0.0127\n",
      "Ep 057 | beta=0.483 | train R 0.7591 KL 0.1788 P 0.0069 | val R 1.1965 KL 0.4232 P 0.0116\n",
      "Ep 058 | beta=0.492 | train R 0.8873 KL 0.2511 P 0.0065 | val R 1.1746 KL 0.3942 P 0.0126\n",
      "Ep 059 | beta=0.500 | train R 0.8474 KL 0.1712 P 0.0063 | val R 1.1334 KL 0.4423 P 0.0103\n",
      "Ep 060 | beta=0.508 | train R 0.8804 KL 0.1764 P 0.0090 | val R 1.1987 KL 0.4268 P 0.0133\n",
      "Ep 061 | beta=0.517 | train R 0.8457 KL 0.1735 P 0.0053 | val R 1.2847 KL 0.4527 P 0.0097\n",
      "Ep 062 | beta=0.525 | train R 0.9947 KL 0.2119 P 0.0089 | val R 1.2684 KL 0.4913 P 0.0114\n",
      "Ep 063 | beta=0.533 | train R 0.8583 KL 0.2022 P 0.0079 | val R 1.2955 KL 0.4149 P 0.0165\n",
      "Ep 064 | beta=0.542 | train R 0.9216 KL 0.1642 P 0.0097 | val R 1.2669 KL 0.4819 P 0.0133\n",
      "Ep 065 | beta=0.550 | train R 0.8056 KL 0.1401 P 0.0052 | val R 1.3062 KL 0.3862 P 0.0162\n",
      "Ep 066 | beta=0.558 | train R 0.9995 KL 0.1545 P 0.0063 | val R 1.3262 KL 0.3850 P 0.0055\n",
      "Ep 067 | beta=0.567 | train R 0.8820 KL 0.2027 P 0.0060 | val R 1.3614 KL 0.3323 P 0.0148\n",
      "Ep 068 | beta=0.575 | train R 0.8788 KL 0.1267 P 0.0062 | val R 1.3559 KL 0.4828 P 0.0097\n",
      "Ep 069 | beta=0.583 | train R 0.9538 KL 0.1505 P 0.0058 | val R 1.3531 KL 0.4152 P 0.0123\n",
      "Ep 070 | beta=0.592 | train R 0.8029 KL 0.1105 P 0.0073 | val R 1.3706 KL 0.3764 P 0.0137\n",
      "Ep 071 | beta=0.600 | train R 1.0378 KL 0.0850 P 0.0065 | val R 1.1806 KL 0.3669 P 0.0092\n",
      "Ep 072 | beta=0.608 | train R 0.8146 KL 0.0796 P 0.0080 | val R 1.3504 KL 0.3965 P 0.0132\n",
      "Ep 073 | beta=0.617 | train R 0.8396 KL 0.0774 P 0.0070 | val R 1.2835 KL 0.4077 P 0.0124\n",
      "Ep 074 | beta=0.625 | train R 1.2035 KL 0.1485 P 0.0122 | val R 1.2297 KL 0.3999 P 0.0113\n",
      "Ep 075 | beta=0.633 | train R 0.9955 KL 0.0778 P 0.0096 | val R 1.2688 KL 0.4500 P 0.0152\n",
      "Ep 076 | beta=0.642 | train R 0.8644 KL 0.0904 P 0.0064 | val R 1.2937 KL 0.4210 P 0.0139\n",
      "Ep 077 | beta=0.650 | train R 0.8682 KL 0.0634 P 0.0097 | val R 1.2679 KL 0.4320 P 0.0099\n",
      "Ep 078 | beta=0.658 | train R 0.8391 KL 0.0778 P 0.0076 | val R 1.2719 KL 0.4210 P 0.0093\n",
      "Ep 079 | beta=0.667 | train R 0.8476 KL 0.0573 P 0.0055 | val R 1.2680 KL 0.4250 P 0.0112\n",
      "Ep 080 | beta=0.675 | train R 0.8657 KL 0.0501 P 0.0076 | val R 1.2870 KL 0.4388 P 0.0132\n",
      "Ep 081 | beta=0.683 | train R 1.0212 KL 0.0802 P 0.0057 | val R 1.2591 KL 0.4194 P 0.0118\n",
      "Ep 082 | beta=0.692 | train R 1.1507 KL 0.0624 P 0.0070 | val R 1.3529 KL 0.4393 P 0.0109\n",
      "Ep 083 | beta=0.700 | train R 0.8440 KL 0.0566 P 0.0088 | val R 1.3004 KL 0.4394 P 0.0140\n",
      "Ep 084 | beta=0.708 | train R 0.8460 KL 0.0498 P 0.0079 | val R 1.2972 KL 0.4886 P 0.0064\n",
      "Ep 085 | beta=0.717 | train R 0.9333 KL 0.0470 P 0.0046 | val R 1.2722 KL 0.4896 P 0.0087\n",
      "Ep 086 | beta=0.725 | train R 0.8299 KL 0.0373 P 0.0042 | val R 1.3043 KL 0.5604 P 0.0058\n",
      "Ep 087 | beta=0.733 | train R 1.1015 KL 0.0638 P 0.0052 | val R 1.2811 KL 0.4729 P 0.0075\n",
      "Ep 088 | beta=0.742 | train R 0.8164 KL 0.0446 P 0.0046 | val R 1.2209 KL 0.4394 P 0.0088\n",
      "Ep 089 | beta=0.750 | train R 0.8830 KL 0.0426 P 0.0048 | val R 1.2532 KL 0.5673 P 0.0101\n",
      "Ep 090 | beta=0.758 | train R 1.0385 KL 0.0415 P 0.0050 | val R 1.2833 KL 0.4941 P 0.0097\n",
      "Ep 091 | beta=0.767 | train R 0.8750 KL 0.0638 P 0.0051 | val R 1.2711 KL 0.5041 P 0.0081\n",
      "Ep 092 | beta=0.775 | train R 0.8546 KL 0.0544 P 0.0049 | val R 1.2504 KL 0.6411 P 0.0110\n",
      "Ep 093 | beta=0.783 | train R 0.8823 KL 0.0565 P 0.0044 | val R 1.2754 KL 0.5219 P 0.0067\n",
      "Ep 094 | beta=0.792 | train R 0.8452 KL 0.0466 P 0.0048 | val R 1.2331 KL 0.5499 P 0.0106\n",
      "Ep 095 | beta=0.800 | train R 0.8565 KL 0.0365 P 0.0049 | val R 1.2722 KL 0.5723 P 0.0132\n",
      "Ep 096 | beta=0.808 | train R 0.9976 KL 0.0545 P 0.0049 | val R 1.2788 KL 0.4792 P 0.0113\n",
      "Ep 097 | beta=0.817 | train R 1.0132 KL 0.0671 P 0.0102 | val R 1.3299 KL 0.5557 P 0.0186\n",
      "Ep 098 | beta=0.825 | train R 0.8665 KL 0.0389 P 0.0047 | val R 1.3054 KL 0.6056 P 0.0094\n",
      "Ep 099 | beta=0.833 | train R 0.9890 KL 0.0306 P 0.0051 | val R 1.2712 KL 0.5996 P 0.0140\n",
      "Ep 100 | beta=0.842 | train R 0.9294 KL 0.0532 P 0.0040 | val R 1.2675 KL 0.5454 P 0.0091\n",
      "Ep 101 | beta=0.850 | train R 0.8592 KL 0.0298 P 0.0053 | val R 1.2082 KL 0.5570 P 0.0064\n",
      "Ep 102 | beta=0.858 | train R 0.8059 KL 0.0334 P 0.0046 | val R 1.2841 KL 0.5823 P 0.0112\n",
      "Ep 103 | beta=0.867 | train R 0.8278 KL 0.0369 P 0.0050 | val R 1.2503 KL 0.5829 P 0.0064\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = np.load(\"../data/processed/data_splits.npz\")  # <- paths per your repo\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "    x_dim, y_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "    train_loader = DataLoader(TabDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "    val_loader   = DataLoader(TabDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "    model = CVAE(x_dim=x_dim, y_dim=y_dim, z_dim=4, hidden=128).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_val = 1e9; patience, bad = 60, 0\n",
    "    EPOCHS = 200\n",
    "    for epoch in range(EPOCHS):\n",
    "        tr_recon, tr_kl, tr_prop, beta = train_epoch(model, train_loader, opt, device,\n",
    "                                                     epoch, warmup_epochs=120, beta_max=1.0, lambda_prop=1.0)\n",
    "        va_recon, va_kl, va_prop = eval_epoch(model, val_loader, device, lambda_prop=1.0)\n",
    "        print(f\"Ep {epoch:03d} | beta={beta:.3f} | \"\n",
    "              f\"train R {tr_recon:.4f} KL {tr_kl:.4f} P {tr_prop:.4f} | \"\n",
    "              f\"val R {va_recon:.4f} KL {va_kl:.4f} P {va_prop:.4f}\")\n",
    "\n",
    "        # Early stop on (recon + prop) to respect both\n",
    "        val_score = va_recon + va_prop\n",
    "        if val_score < best_val - 1e-5:\n",
    "            best_val = val_score; bad = 0\n",
    "            torch.save(model.state_dict(), \"../models/cvae_best.pt\")\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\"); break\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
