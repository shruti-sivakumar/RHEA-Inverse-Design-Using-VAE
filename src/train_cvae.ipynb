{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0da7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, joblib, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvae import CVAE, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b25903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X = torch.tensor(X, dtype=torch.float32); self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54415a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, device, epoch, total_epochs, beta_max=1.0, warmup_epochs=50):\n",
    "    model.train()\n",
    "    recon_losses, kl_losses = [], []\n",
    "    # KL annealing\n",
    "    if epoch < warmup_epochs:\n",
    "        beta = beta_max * (epoch+1) / warmup_epochs\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        loss = recon + beta * kl\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses), beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    recon_losses, kl_losses = [], []\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        X_hat, (q_mu, q_logvar, p_mu, p_logvar) = model(Xb, yb)\n",
    "        recon = nn.functional.mse_loss(X_hat, Xb, reduction=\"mean\")\n",
    "        kl = kl_divergence(q_mu, q_logvar, p_mu, p_logvar)\n",
    "        recon_losses.append(recon.item()); kl_losses.append(kl.item())\n",
    "    return np.mean(recon_losses), np.mean(kl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c316473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | beta=0.013 | train recon 0.9876 KL 0.3145 | val recon 1.6971 KL 0.5850\n",
      "Epoch 001 | beta=0.025 | train recon 0.9244 KL 0.9903 | val recon 1.6805 KL 1.3679\n",
      "Epoch 002 | beta=0.037 | train recon 0.8144 KL 2.4619 | val recon 1.6211 KL 1.7434\n",
      "Epoch 003 | beta=0.050 | train recon 0.6565 KL 2.3806 | val recon 1.6080 KL 1.5788\n",
      "Epoch 004 | beta=0.062 | train recon 0.6531 KL 2.1718 | val recon 1.5561 KL 1.3928\n",
      "Epoch 005 | beta=0.075 | train recon 0.6192 KL 1.9079 | val recon 1.5928 KL 1.5939\n",
      "Epoch 006 | beta=0.087 | train recon 0.6111 KL 2.0559 | val recon 1.5828 KL 1.7814\n",
      "Epoch 007 | beta=0.100 | train recon 0.5130 KL 2.0628 | val recon 1.5265 KL 1.5912\n",
      "Epoch 008 | beta=0.113 | train recon 0.5245 KL 2.2858 | val recon 1.4926 KL 1.3846\n",
      "Epoch 009 | beta=0.125 | train recon 0.5262 KL 1.7573 | val recon 1.5475 KL 1.1256\n",
      "Epoch 010 | beta=0.138 | train recon 0.6017 KL 1.4892 | val recon 1.5504 KL 1.1089\n",
      "Epoch 011 | beta=0.150 | train recon 0.5171 KL 1.5621 | val recon 1.3829 KL 1.0969\n",
      "Epoch 012 | beta=0.163 | train recon 0.5827 KL 1.9047 | val recon 1.3440 KL 1.0324\n",
      "Epoch 013 | beta=0.175 | train recon 0.4955 KL 1.3284 | val recon 1.3990 KL 1.0012\n",
      "Epoch 014 | beta=0.188 | train recon 0.5338 KL 1.1919 | val recon 1.4829 KL 0.9335\n",
      "Epoch 015 | beta=0.200 | train recon 0.4806 KL 1.4730 | val recon 1.3621 KL 0.8696\n",
      "Epoch 016 | beta=0.212 | train recon 0.5384 KL 1.1776 | val recon 1.3812 KL 0.8574\n",
      "Epoch 017 | beta=0.225 | train recon 0.5192 KL 1.1117 | val recon 1.5858 KL 0.8004\n",
      "Epoch 018 | beta=0.237 | train recon 0.7293 KL 1.3391 | val recon 1.3040 KL 0.8101\n",
      "Epoch 019 | beta=0.250 | train recon 0.5969 KL 1.0591 | val recon 1.3068 KL 0.7285\n",
      "Epoch 020 | beta=0.263 | train recon 0.5119 KL 0.9885 | val recon 1.4478 KL 0.6787\n",
      "Epoch 021 | beta=0.275 | train recon 0.5390 KL 1.0313 | val recon 1.1426 KL 0.6997\n",
      "Epoch 022 | beta=0.287 | train recon 0.5355 KL 0.8622 | val recon 1.5686 KL 0.6136\n",
      "Epoch 023 | beta=0.300 | train recon 0.5320 KL 0.7740 | val recon 1.2351 KL 0.5128\n",
      "Epoch 024 | beta=0.312 | train recon 0.6140 KL 0.7749 | val recon 1.4961 KL 0.4241\n",
      "Epoch 025 | beta=0.325 | train recon 0.5900 KL 0.7202 | val recon 1.2867 KL 0.3845\n",
      "Epoch 026 | beta=0.338 | train recon 0.6589 KL 0.6303 | val recon 1.2622 KL 0.4092\n",
      "Epoch 027 | beta=0.350 | train recon 0.7367 KL 0.9702 | val recon 0.9284 KL 0.4926\n",
      "Epoch 028 | beta=0.362 | train recon 0.6652 KL 0.6911 | val recon 1.2987 KL 0.4461\n",
      "Epoch 029 | beta=0.375 | train recon 0.7215 KL 0.6562 | val recon 1.0749 KL 0.3869\n",
      "Epoch 030 | beta=0.388 | train recon 0.6295 KL 0.6147 | val recon 1.5255 KL 0.3223\n",
      "Epoch 031 | beta=0.400 | train recon 0.7217 KL 0.5373 | val recon 1.2783 KL 0.2983\n",
      "Epoch 032 | beta=0.412 | train recon 0.6369 KL 0.5149 | val recon 1.5250 KL 0.2848\n",
      "Epoch 033 | beta=0.425 | train recon 0.7388 KL 0.4434 | val recon 1.4972 KL 0.2759\n",
      "Epoch 034 | beta=0.438 | train recon 0.6768 KL 0.3893 | val recon 1.3351 KL 0.2804\n",
      "Epoch 035 | beta=0.450 | train recon 1.0390 KL 0.3789 | val recon 1.1178 KL 0.2829\n",
      "Epoch 036 | beta=0.463 | train recon 0.7358 KL 0.3479 | val recon 1.5535 KL 0.2880\n",
      "Epoch 037 | beta=0.475 | train recon 0.7216 KL 0.3985 | val recon 1.3755 KL 0.2986\n",
      "Epoch 038 | beta=0.487 | train recon 0.7290 KL 0.3840 | val recon 1.1482 KL 0.2776\n",
      "Epoch 039 | beta=0.500 | train recon 0.7548 KL 0.3483 | val recon 1.0015 KL 0.2751\n",
      "Epoch 040 | beta=0.512 | train recon 0.9938 KL 0.4898 | val recon 1.1154 KL 0.2619\n",
      "Epoch 041 | beta=0.525 | train recon 0.6568 KL 0.3022 | val recon 1.4984 KL 0.2435\n",
      "Epoch 042 | beta=0.537 | train recon 0.6995 KL 0.2802 | val recon 1.3510 KL 0.2132\n",
      "Epoch 043 | beta=0.550 | train recon 0.7467 KL 0.2482 | val recon 1.3555 KL 0.2204\n",
      "Epoch 044 | beta=0.562 | train recon 0.7873 KL 0.2089 | val recon 1.3233 KL 0.1972\n",
      "Epoch 045 | beta=0.575 | train recon 0.9742 KL 0.2612 | val recon 1.2728 KL 0.2267\n",
      "Epoch 046 | beta=0.588 | train recon 0.7282 KL 0.2288 | val recon 1.3301 KL 0.2229\n",
      "Epoch 047 | beta=0.600 | train recon 0.8146 KL 0.2343 | val recon 1.4452 KL 0.2006\n",
      "Epoch 048 | beta=0.613 | train recon 0.7852 KL 0.2521 | val recon 1.0770 KL 0.1924\n",
      "Epoch 049 | beta=0.625 | train recon 0.7749 KL 0.2183 | val recon 1.3560 KL 0.2143\n",
      "Epoch 050 | beta=0.637 | train recon 0.7474 KL 0.2179 | val recon 0.9870 KL 0.2531\n",
      "Epoch 051 | beta=0.650 | train recon 0.7403 KL 0.1975 | val recon 1.6718 KL 0.1945\n",
      "Epoch 052 | beta=0.662 | train recon 0.8125 KL 0.1763 | val recon 1.7046 KL 0.1666\n",
      "Epoch 053 | beta=0.675 | train recon 1.0173 KL 0.1863 | val recon 1.4171 KL 0.1654\n",
      "Epoch 054 | beta=0.688 | train recon 0.9392 KL 0.1625 | val recon 1.4496 KL 0.1610\n",
      "Epoch 055 | beta=0.700 | train recon 0.6837 KL 0.1347 | val recon 1.5908 KL 0.1716\n",
      "Epoch 056 | beta=0.713 | train recon 0.8262 KL 0.1236 | val recon 1.1916 KL 0.2033\n",
      "Epoch 057 | beta=0.725 | train recon 0.9166 KL 0.1674 | val recon 1.5431 KL 0.2133\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load splits\n",
    "    data = np.load(\"../data/processed/data_splits.npz\")\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "    x_dim, y_dim = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(TabDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "    val_loader   = DataLoader(TabDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = CVAE(x_dim=9, y_dim=1, z_dim=6, hidden=128).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_val = 1e9; patience, bad = 30, 0\n",
    "    for epoch in range(300):\n",
    "        tr_recon, tr_kl, beta = train_epoch(model, train_loader, opt, device, epoch, 300, beta_max=1.0, warmup_epochs=80)\n",
    "        va_recon, va_kl = eval_epoch(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch:03d} | beta={beta:.3f} | train recon {tr_recon:.4f} KL {tr_kl:.4f} | val recon {va_recon:.4f} KL {va_kl:.4f}\")\n",
    "\n",
    "        # Early stopping on val recon\n",
    "        if va_recon < best_val - 1e-5:\n",
    "            best_val = va_recon\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), \"cvae_best.pt\")\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience: \n",
    "            print(\"Early stopping.\"); break\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
